\chapter{Grundlagen}
\label{ch:Grundlagen}

Dieses Kapitel legt die Grundlagen für künstliche neuronale Netze dar, indem es detailliert auf ihre Struktur und Funktionsweise eingeht, mit Fokus auf vorwärtsgerichtete Netze. Dabei wird ein umfassender Überblick über die potenziellen Anwendungsbereiche von künstlichen neuronalen Netzen gegeben.

Des Weiteren wird die Thematik der Parallelisierung sowohl im allgemeinen Kontext als auch speziell im Zusammenhang mit neuronalen Netzen erläutert. Es werden die Vor- und Nachteile dieser Technik beleuchtet und die gängigsten Methoden zur Parallelisierung von Berechnungen in neuronalen Netzen werden ausführlich diskutiert.
Die Nutzung von Grafikprozessoren (GPUs) für parallele Berechnungen wird dabei angesprochen, jedoch liegt der Fokus auf den grundlegenden Prinzipien der Parallelisierung von neuronalen Netzen und deren Implementierung mittels Thread- und Prozessparallelsierung.

\section{Grundprinzipien von neuronalen Netzen}
\label{sec:Grundlagen_Grundprinzipien_neuronale_Netze}
Neuronale Netze sind ein wesentlicher Bestandteil des maschinellen Lernens und der künstlichen Intelligenz. Sie sind inspiriert von der Funktionsweise des menschlichen Gehirns und bestehen aus einer Ansammlung miteinander verbundener Knoten, die genau wie beim menschlichen Gehirn als Neuronen bezeichnet werden \citep{Manuela_Kunstliche_Intelligenz}.

Ein mehrlagiges neuronales Netz ist in verschiedene Schichten organisiert, die jeweils spezifische Funktionen erfüllen. Die erste Schicht wird oft als Eingangsschicht bezeichnet und empfängt die Rohdaten oder Merkmale, die dem Netz präsentiert werden. Diese Daten werden dann durch das Netz weitergeleitet, wobei jede Schicht eine spezifische Transformation durchführt.
Zwischen der Eingangsschicht und der Ausgangsschicht können mehrere versteckte Schichten vorhanden sein. Diese versteckten Schichten sind entscheidend für die Fähigkeit des Netzes, komplexe Muster zu lernen und abstrakte Merkmale zu extrahieren. Jede Schicht lernt auf unterschiedlichen Abstraktionsebenen und trägt zur schrittweisen Verbesserung der Leistung des Netzes bei.

Die Funktionsweise eines neuronalen Netzes lässt sich allgemein in zwei Hauptphasen unterteilen: Vorwärtspropagierung und Rückwärtspropagierung. Während der Vorwärtspropagierung fließen die Eingabedaten vorwärts durch das Netz, beginnend mit den Neuronen der Eingabeschicht, welche die Rohdaten empfangen, und endend mit den Neuronen der Ausgabeschicht, welche die Vorhersagen oder Klassifikationen des Netzes abbilden. Jedes Neuron ist mit anderen Neuronen verbunden, und diese Verbindungen sind mit Gewichten versehen, die die Stärke der Verbindung zwischen den Neuronen darstellen  \citep{Manuela_Kunstliche_Intelligenz}.

Während der Vorwärtspropagierung durchläuft jede Eingabe eine Reihe von Schichten im Netz, wobei jede Schicht aus einer bestimmten Anzahl von Neuronen besteht. Jedes Neuron einer Schicht erhält Eingaben von den Neuronen der vorherigen Schicht, multipliziert diese Eingaben mit den entsprechenden Gewichten und summiert sie, wie in Gleichung \ref{eq:perceptron_input_eq} dargestellt. Anschließend wird wie in Gleichung \ref{eq:perceptron_output_eq} definiert eine Aktivierungsfunktion auf die gewichtete Summe angewendet, um die Ausgabe des Neurons zu berechnen, die dann an die Neuronen der nächsten Schicht weitergeleitet wird \citep{thesis_Artur_Brening}.

\begin{eqnarray}
    Eingabe&=&w_0b+\sum_{i=1}^{n}w_ix_i \label{eq:perceptron_input_eq} \\
    Ausgabe&=&f_a(Eingabe) \label{eq:perceptron_output_eq}
\end{eqnarray}

\begin{center}
    \begin{footnotesize}
        $w_0$: zugehöriges Gewicht; $b$: Schwellenwert; $w_i$: zugehörige Gewichte; $x_i$: Eingabewerte\\
        $f_a$: Aktivierungsfunktion
    \end{footnotesize}
\end{center}

Die Ausgangsschicht liefert schließlich die Ergebnisse der Netzberechnungen, sei es in Form einer Klassifikation, Regression oder einer anderen Art der Informationsverarbeitung, je nach den Anforderungen der spezifischen Anwendung. Durch die Strukturierung des Netzes in Schichten und die Festlegung spezifischer Funktionen für jede Schicht kann das neuronale Netz effizient Informationen verarbeiten und komplexe Probleme lösen.

Die Rückwärtspropagierung ist der Prozess, bei dem das Netz lernt, indem es seine Gewichte entsprechend der Fehler zwischen den tatsächlichen und den vorhergesagten Ausgaben anpasst. Dies geschieht durch die Berechnung von Gradienten zum Beispiel mit Hilfe des Backpropagation-Algorithmus \citep{thesis_Artur_Brening} und die Anpassung der Gewichte mithilfe eines Optimierungsalgorithmus wie dem Gradientenabstiegsverfahren.

\section{Einführung in die Parallelisierung}
\label{sec:Grundlagen_Parallelisierung}
Die Parallellisierung stellt einen zentralen Ansatz dar, um die Leistungsfähigkeit von Computersystemen zu steigern, insbesondere angesichts der Tatsache, dass moderne CPUs und GPUs über eine wachsende Anzahl von Kernen verfügen. Kernpunkt der Parallellisierung ist die simultane und unabhängige Ausführung von Aufgaben oder Berechnungen, anstatt einer sequenziellen Abfolge. Dieser Ansatz findet breite Anwendung in verschiedenen Bereichen wie High-Performance-Computing (HPC), Datenverarbeitung, Simulationen, künstlicher Intelligenz und weiteren \citep{Flynn_Computer_Organizations_and_their_Effectiveness}.

Eine Vielzahl von Herangehensweisen zur Parallellisierung existiert, die abhängig von der Problemstellung und der verfügbaren Hardware eingesetzt werden können. Die Task-Parallellisierung zielt darauf ab, Aufgaben auf mehrere Prozessoren oder Kerne zu verteilen. Insbesondere für Anwendungen mit vielen simultan auszuführenden Aufgaben wie parallele Suchalgorithmen oder Simulationen von physikalischen Systemen eignet sich diese Art der Parallellisierung besonders \citep{Flynn_Computer_Organizations_and_their_Effectiveness}.

Ein weiterer Ansatz ist die Datenparallellisierung, bei der ein Problem in kleinere Teile zerlegt wird, die jeweils auf unterschiedlichen Datensätzen operieren. Dieser Ansatz ist besonders effektiv für Anwendungen, die eine simultane Verarbeitung großer Datenmengen erfordern, wie beispielsweise Bildverarbeitung oder maschinelles Lernen \citep{Flynn_Computer_Organizations_and_their_Effectiveness}.

Es ist jedoch zu betonen, dass nicht alle Probleme gleichermaßen für eine Parallellisierung geeignet sind. Manche Probleme beinhalten intrinsische Abhängigkeiten oder Sequenzialität, die eine effektive Parallellisierung erschweren oder gar unmöglich machen.

\subsection{Vor- und Nachteile von Parallelisierung}
\label{sec:Grundlagen_Parallelisierung_Vorteile_Nachteile}
Die Parallelisierung bietet eine Vielzahl von Vorteilen, die zur Leistungssteigerung von Computersystemen beitragen. Einer der offensichtlichsten Vorteile ist die Verbesserung der Ausführungsgeschwindigkeit von Programmen und Berechnungen durch die gleichzeitige Ausführung von Aufgaben oder die Verarbeitung von Daten auf mehreren Prozessoren oder Kernen. Diese beschleunigte Ausführung ist insbesondere bei rechenintensiven Anwendungen von Vorteil \citep{Flynn_Computer_Organizations_and_their_Effectiveness}.

Ein zusätzlicher Vorteil der Parallelisierung liegt in ihrer Skalierbarkeit, da Aufgaben oder Daten auf mehrere Ressourcen aufgeteilt werden können, um Systeme leichter an wachsende Anforderungen anzupassen. Dies ermöglicht es, die Leistungsfähigkeit von Systemen flexibel zu erweitern, ohne dass eine komplette Neuentwicklung erforderlich ist \citep{Flynn_Computer_Organizations_and_their_Effectiveness}.
Des Weiteren kann die Parallelisierung die Auslastung von Ressourcen optimieren und Engpässe reduzieren, indem sie Prozessoren oder andere Hardware-Ressourcen effizient nutzt. Dies trägt dazu bei, die Gesamtleistung des Systems zu verbessern. Eine effiziente Nutzung der verfügbaren Hardware ist nicht nur im Bezug der Systemleistung vorteilhaft zu bewerten, sondern ermöglicht es auch, mehr Arbeit auf wenigeren Systemen auszuführen, da das volle Leistungspotenzial aller Systeme ausgenutzt wird \citep{Flynn_Computer_Organizations_and_their_Effectiveness}. So werden auch Kosten gesenkt.

Trotz dieser Vorteile gibt es auch einige Nachteile und Herausforderungen bei der Implementierung von Parallelisierung. Ein wichtiger Aspekt sind die erhöhten Anforderungen an die Programmierung und die Algorithmuskonzeption, da die Entwicklung paralleler Algorithmen und die Verwaltung paralleler Prozesse spezifisches Fachwissen erfordern. Darüber hinaus können Probleme wie Datenabhängigkeiten, Wettlaufsituationen und Synchronisationskonflikte auftreten, die die Entwicklung und Fehlerbehebung erschweren. Parallele Implementierungen sind fast ausschließlich komplexer als ihre sequenziellen Pendants. Es gibt einige Ansätze, die Parallelisierung dem Programmierer gegenüber transparent zu gestalten \citep{Sidorenko_Subway_Train_Scheduling}, jedoch stellten sich diese Bemühungen größtenteils als erfolglos heraus \citep{the_problem_with_threads}, da sie oft unvorhersehbare Verhaltensweisen mit sich bringen und das Debugging erschweren. 

Ein weiterer Nachteil ist die potenzielle Zunahme des Energieverbrauchs, insbesondere wenn die Parallelisierung nicht effizient implementiert ist. Dies ist besonders relevant in Umgebungen, in denen Energieeffizienz ein wichtiges Anliegen ist, wie beispielsweise in mobilen Geräten oder Rechenzentren.

\subsection{Parallelisierung in vorwärtsgerichteten Netzen}
\label{sec:Grundlagen_Parallelisierung_Neuronale_Netze}
Für die parallele Ausführung des Trainings mehrerer Netze unabhängig voneinander spielt die Thread- und Prozessparallelisierung eine bedeutende Rolle. Diese Techniken bieten Mechanismen, um das Training der Netze auf mehrere Threads oder Prozesse aufzuteilen, was die Effizienz und Geschwindigkeit des Trainings verbessern kann \citep{Flynn_Computer_Organizations_and_their_Effectiveness}.

Thread-Parallelisierung bezieht sich auf die Aufteilung des Trainingsprozesses eines Netzs in mehrere Threads, die gleichzeitig auf einem einzigen Prozessorkern oder auf mehreren Kernen eines Mehrkernprozessors ausgeführt werden können. In diesem Szenario ermöglicht die Thread-Parallelisierung das gleichzeitige Training mehrerer Netze, wobei jeder Thread sich auf das Training eines bestimmten Netzs konzentriert. Dies kann die Gesamttrainingsdauer reduzieren und die Auslastung der verfügbaren Prozessorressourcen optimieren \citep{Flynn_Computer_Organizations_and_their_Effectiveness}.

Prozessparallelisierung hingegen umfasst die Aufteilung des Trainingsprozesses in mehrere unabhängige Prozesse, die auf verschiedenen Prozessorkernen oder sogar auf verschiedenen physikalischen Maschinen ausgeführt werden können. Bei der Prozessparallelisierung werden die Trainingsvorgänge mehrerer Netze auf separaten Prozessen ausgeführt, was eine hochgradig parallele Verarbeitung und Skalierbarkeit über mehrere Computerknoten hinweg ermöglicht. Die Kommunikation zwischen den Prozessen kann über verschiedene Mechanismen wie Sockets, Messaging-Systeme oder gemeinsam genutzte Speicherbereiche erfolgen \citep{Flynn_Computer_Organizations_and_their_Effectiveness}.

Die Parallelisierung des Trainings und Testens von neuronalen Netzen mittels Grafikprozessoren (GPUs) hat sich als eine zentrale Methode zur Steigerung der Effizienz und Geschwindigkeit in der modernen maschinellen Lernforschung etabliert. GPUs sind besonders gut für die Verarbeitung der massiven parallelen Berechnungen geeignet, die beim Training großer neuronaler Netze erforderlich sind \citep{scalable_parallel_programming_with_cuda}. GPUs enthalten eine große Anzahl von Kernen, die für das gleichzeitige Ausführen viele Operationen konzipiert sind. Dies steht im Gegensatz zu den wenigen, dafür aber leistungsstärkeren Kernen einer CPU. Bei neuronalen Netzen bestehen die Berechnungen häufig aus Matrix-Multiplikationen und anderen linear-algebraischen Operationen, die sich gut parallelisieren lassen \citep{scalable_parallel_programming_with_cuda}. Auf diese Parallelisierungstechnik wird in dieser Arbeit jedoch nicht weiter eingegangen, da der Fokus auf der Thread-Parallelisierung liegt.

\subsection{Implementierung von Parallelisierungstechniken}
\label{sec:Grundlagen_Parallelisierung_Implementierung}
Datenparallelismus ist eine der am häufigsten verwendeten Techniken zur Parallelisierung neuronaler Netze. Der Datensatz wird in kleinere Datensätze aufgeteilt, die gleichzeitig auf verschiedenen Recheneinheiten verarbeitet werden. Jeder Thread führt die gleichen Operationen auf unterschiedlichen Datenpartitionen durch, und die Ergebnisse werden anschließend kombiniert \citep{pytorch_advances_in_neural_systems}.

Beim Modellparallelismus wird das neuronale Netz selbst in verschiedene Teile aufgeteilt, die gleichzeitig auf verschiedenen Recheneinheiten ausgeführt werden. Diese Parallelisierungstechnik wird zum Beispiel häufig verwendet, wenn das Modell zu groß ist, um auf einer einzelnen GPU oder einem einzelnen Knoten zu laufen. Jede Recheneinheit berechnet einen Teil des Modells, und die Ergebnisse werden zur Berechnung der nächsten Schicht kombiniert.

Für die Implementierung von neuronalen Netzen kommen heutzutage häufig Bibliotheken wie PyTorch oder TensorFlow zum Einsatz. Diese bieten Implementierungen von vielen Algorithmen für neuronale Netze, die von den angesprochenen Parallelisierungstechniken Gebrauch machen, was die Implementierung von parallelisierten und performanten neuronalen Netzen deutlich vereinfacht hat \citep{pytorch_advances_in_neural_systems}.
Diese Bibliotheken nehmen dem Entwickler viel Arbeit ab, erlauben jedoch keine genaue Kontrolle über die Parallelisierung.

Eine weitere Strategie ist es, verschiedene Threads zu nutzen um dasselbe Netz mehrfach laufen zu lassen. Dies ermöglicht beispielsweise die gleichzeitige Inferenz mit verschiedenen Eingabewerten, wenn man ein bereits trainiertes Netz hat. Alternativ ist es so auch möglich, den Trainingsablauf neuronaler Netze zu parallelisieren, indem man mehrere Netze mit verschiedenen Zufallszahlen gleichzeitig laufen lässt. Die Auswirkungen dieses Ansatzes auf die Leistungsfähigkeit werden im folgenden Abschnitt behandelt.

\subsection{Auswirkungen auf die Leistungsfähigkeit}
\label{sec:Grundlagen_Parallelisierung_Leistungsfähigkeit}

Die Parallelisierung von Algorithmen neuronaler Netze kann, wie bereits beschrieben, zu einer signifikanten Reduktion der Laufzeit führen, indem mehrere Threads gleichzeitig ausgeführt werden. Die potenzielle Verbesserung der Laufzeit allgemeiner Algorithmen bei der Verwendung von $n$ Threads wird durch das Gesetz von Amdahl beschrieben. Formel \ref{eq:amdahls_law} zeigt, wie die Laufzeit $T(n)$ bei Verwendung von $n$ Threads berechnet wird. Dieses Gesetz berücksichtigt den Anteil $S$ des Programms, der nicht parallelisiert werden kann, und ermöglicht die Berechnung der tatsächlichen Laufzeit bei paralleler Ausführung. \citep{amdahl_validity_single_processor}.

\begin{eqnarray}
    T(n) &=& \frac{T(1)}{S + \frac{1 - S}{n}} \label{eq:amdahls_law}
\end{eqnarray}

\begin{center}
    \begin{footnotesize}
        Gesetz von Amdahl (Amdahl's Law) \\
        $T(n)$: Laufzeit bei Verwendung von $n$ Threads; $T(1)$: sequentielle Laufzeit mit 1 Thread;\\$S$: Anteil der Laufzeit des Programms, der nicht parallelisiert werden kan
    \end{footnotesize}
\end{center}

Dieses Gesetz trifft auch auf die möglichen Verbesserungen der Laufzeit im Kontext neuronaler Netze zu. So besteht ein wesentlicher Vorteil der parallelen Ausführung mehrerer Netze (mit beispielsweise verschiedenen Zufalls-Seeds) in der Vielzahl von Trainingsdurchläufen, die simultan durchgeführt werden können. Dies ermöglicht es, eine breite Palette von Modellen zu trainieren und verschiedene Variationen zu erkunden, um letztendlich das optimale Modell zu identifizieren. Durch die gleichzeitige Ausführung dieser Trainingsläufe können Entwickler Zeit sparen und schneller zu aussagekräftigen Ergebnissen gelangen \citep{Practical_neural_network_recipes_C++}.

Außerdem bietet die parallele Ausführung die Möglichkeit, Inferenzoperationen gleichzeitig durchzuführen. Mehrere Eingaben können gleichzeitig an duplizierte Netze weitergeleitet werden, um eine simultane Auswertung zu ermöglichen. Dies beschleunigt nicht nur den Inferenzprozess erheblich, sondern ermöglicht auch eine effizientere Nutzung der verfügbaren Hardwareressourcen. So können beispielsweise die Eingaben mehrerer Benutzer gleichzeitig verarbeitet werden.

Ein weiterer Vorteil besteht in der verbesserten Skalierbarkeit der Anwendung. Durch die Nutzung von Thread- oder Prozessparallelisierung kann die Anwendung problemlos auf mehreren Rechenknoten oder sogar in Cloud-Umgebungen skaliert werden. Dies ermöglicht es, die Trainings- und Inferenzkapazitäten je nach Bedarf flexibel anzupassen und die Gesamtperformance der Anwendung zu optimieren.
